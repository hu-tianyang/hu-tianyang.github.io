#### Former Members: 

##### Reseach Interns at Huawei Noah's Ark Lab.

- [Yimeng Chen](https://scholar.google.com/citations?user=KalhG8AAAAAJ&hl=en), worked on model ensemble for domain generalization ([ICML 2023](https://proceedings.mlr.press/v202/chen23m.html)), while pursuing a PhD at the University of Chinese Academy of Sciences.

- [Pawe≈Ç Piwek](https://www.linkedin.com/in/pawe%C5%82-piwek-99243b169/?originalSubdomain=uk), worked on boundary complexity for ReLU classifiers ([UAI 2023](https://proceedings.mlr.press/v216/piwek23a/piwek23a.pdf)), while pursuing a PhD at the University of Oxford.

- [Yi Liu](https://peterlau61.github.io/), worked on optimal latent space for generative modeling, while pursuing a BS at CUHK-Shenzhen.

- [Weijian Luo](https://pkulwj1994.github.io/), worked on diffusion distillation ([NeurIPS 2023](https://proceedings.neurips.cc/paper_files/paper/2023/file/f115f619b62833aadc5acb058975b0e6-Paper-Conference.pdf)), while pursuing a PhD at Peking University.

- [Jiajun Ma](https://scholar.google.com/citations?user=DDhhRooAAAAJ&hl=zh-CN), worked on diffusion sampling acceleration ([ICLR 2024](https://openreview.net/forum?id=9DXXMXnIGm)) and classifier guidance ([ICML 2024](https://proceedings.mlr.press/v235/ma24r.html)), while pursuing a PhD at HKUST-Guangzhou.

- [Xuantong Liu](https://scholar.google.com/citations?user=5hGI8ZoAAAAJ&hl=en), worked on conditional generation via model inversion ([ICML 2024](https://proceedings.mlr.press/v235/liu24aa.html)), while pursuing a PhD at HKUST.

- [Jakub Wornbard](https://www.linkedin.com/in/jakub-wornbard-a59b60195/?originalSubdomain=uk), worked on the duality between prompts and model weights in Transformers, while pursuing a MS at the University of Cambridge.

- [Shuchen Xue](https://scholar.google.com/citations?user=aA70TOwAAAAJ&hl=en), worked on diffusion sampling acceleration ([ICML 2024](https://proceedings.mlr.press/v235/ma24r.html)) and discrete diffusion models, while pursuing a PhD at the University of Chinese Academy of Sciences.

- [Yihong Luo](https://luo-yihong.github.io/), worked on diffusion distillation ([ICLR 2025](https://openreview.net/forum?id=T7bmHkwzS6), [arXiv 2025](https://arxiv.org/abs/2503.06674)) and controllable generation ([arXiv 2025](https://arxiv.org/abs/2503.06652), [arXiv 2025](https://arxiv.org/abs/2503.13070)), while pursuing a PhD at HKUST.

